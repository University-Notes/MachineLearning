\documentclass{article}
\author{Matteo Secco}
\title{Machine Learing}

\usepackage{hyperref, titlesec}
\newcommand{\sectionbreak}{\clearpage}

\usepackage{mathtools, dsfont}
\newcommand{\R}{\mathds{R}}

\begin{document}
\maketitle\newpage
\tableofcontents\newpage


\section{Supervised  Learning}

\paragraph{input x} also called features or attributes

\paragraph{output t} also called targets or labels

\paragraph{Goal} find a good approximation for $f:x \to t$

\subsection{Tasks}
\begin{description}
\item[Classification] t is discrete
\item[Regression] t is continuous
\item[Probability estimation] t is a probability
\end{description}

\subsection{When to use supervised learning?}
\begin{itemize}
\item Human cannot perform the task
\item Human can perform the task but cannot explain how
\item Task changes over time
\item Task is user-specific
\end{itemize}

\subsection{Steps} 
To approximate $f$ over the dataset $\mathcal{D}$
\begin{enumerate}
\item Define a loss function $\mathcal{L}$
\item Choose the hypothesis space $\mathcal{H}$
\item find $h \in \mathcal{H}$ that minimizes $\mathcal{L}$ over $\mathcal{D}$
\end{enumerate}

\subsection{Representation, Evaluation, Optimization}

\paragraph{Examples of representation}
\begin{itemize}
\item Linear models
\item Instance-based
\item Decision trees
\item Set of rules
\item Graphical models
\item Neural networks
\item Gaussian Processes
\item Support vector machines
\item Model ensembles
\end{itemize}

\paragraph{Examples of evaluation}
\begin{itemize}
\item Accuracy
\item Precision and recall
\item Squared Error
\item Likelihood
\item Posterior probability
\item Cost/Utility
\item Margin
\item Entropy
\item KL divergence
\end{itemize}

\paragraph{Examples of optimization}
\begin{itemize}
\item Combinatorial optimization
	\begin{itemize}
	\item e.g.: Greedy search
	\end{itemize}
\item Convex optimization
	\begin{itemize}
	\item e.g.: Gradient descent
	\end{itemize}
\item Constrained optimization
	\begin{itemize}
	\item e.g.: Linear programming
	\end{itemize}
\end{itemize}

\subsection{Supervised learning taxonomy}
\begin{itemize}
\item Parametric vs Nonparametric
\begin{itemize}
	\item Parametric: fixed and finite number of parameters
	\item Nonparametric: the number of parameters depends on thetraining set
\end{itemize}
\item Frequentist vs Bayesian
\begin{itemize}
	\item Frequentist: use probabilities to model the sampling process 
	\item Bayesian: use probability to model uncertainty about the estimate
\end{itemize}
\item Empirical Risk Minimization vs Structural Risk Minimization
\begin{itemize}
	\item Empirical Risk: Error over the training set
	\item Structural Risk: Balance training error with model complexity
\end{itemize}
\item Direct vs Generative vs Discriminative 
\begin{itemize}
	\item Generative: learns the joint probability distribution $p(x,t)$
	\item Discriminative: learns the conditional probability distribution $p(t|x)$
\end{itemize}
\end{itemize}

\subsection{Learning approaches}

\paragraph{Direct approach} Learn directly $f$ from $D$

\paragraph{Discriminative approach}
\begin{itemize}
\item Model $p(t|x)$
\item Marginalize to find ${E[t|x]=\int t\cdot p(t|x)\, dt}$
\end{itemize}

\paragraph{Generative approach}
\begin{itemize}
\item Model $p(x,t)$
\item Infer $p(t|x)$ (Bayes rule)
\item Marginalize to find ${E[t|x]=\int t\cdot p(t|x)\, dt}$
\end{itemize}

\section{Linear  regression}
\paragraph{Regression} Learn an approximation of $f(x):X\to \R$
\begin{itemize}
\item How to model $f$?
\item How to optimize the approximation?
\item How to evaluate the approximation?
\end{itemize}
\paragraph{Linear regression} models $f$ with linear functions
\begin{itemize}
\item easy to explain
\item analytically solvable
\item extendable to model non-linear relations
\item base for more sophisticated models
\end{itemize}
\paragraph{First linear model}
\[
y(\vec{x},\vec{w})=
\underbrace{w_0}_{\text{bias parameter}}
+\sum_{j=1}^{D-1} w_jx_j
=\vec{w}^T \cdot \underbrace{\vec{x}}_{(1,x_1,...,x_{D-1})}
\]
\paragraph{Sum of Squared Errors}
Error loss for linear regression
\[
L(\vec{w})=\frac{1}{2} \underbrace{\sum_{n=1}^N \left[y(x_n,\vec{w})-t_n\right]^2}_{\text{Residual Sum of Squares}}
\]
\paragraph{Residual Sum of Squares}
\[
RSS(\vec{w})=\|\vec{\epsilon}\|_2^2=\sum_{i=1}^N \epsilon_i^2
\]
\subsection{Linear models}
We can define more complex models modeling non linearity: the regression model must be \underline{linear in  the parameters}, but the parameters can be non linear wrt the data.
\paragraph{Basis function} is a function $\phi$ mapping data to parameters:
\[
y(\vec{x},\vec{w})=
w_0
+\sum_{j=1}^{M-1} w_j\phi_j(\vec{x})
=\vec{w}^T \cdot \underbrace{\vec{\phi}(\vec{x})}_{(1,\phi_1(\vec{x}),...,\phi_{M-1}(\vec{x}))}
\]
\paragraph{Some examples of basis functions:}
\begin{description}
\item.[Polynomial:] $\phi_j(x)=x^j$
\item[Gaussian:] $\phi_j(x)=e^{-\frac{(x-\mu_j)^2}{2\sigma^2}}$
\item[Sigmoidal:] $\phi_j(x)=\frac{1}{1+e^{\frac{\mu_j-x}{\sigma}}}$
\end{description}
\paragraph{Least Squares}
\[
L(\vec{w})=\frac{1}{2}RSS(\vec{2})=\frac{1}{2}(\vec{t}-\vec{\phi}\vec{w})^T(\vec{t}-\vec{\phi}\vec{w})
\]
\[
\frac{\partial L(\vec{w})}{\partial \vec{w}}=-\vec{\phi}^T(\vec{t}-\vec{\phi}\vec{w})
\qquad
\frac{\partial^2 L(\vec{w})}{\partial \vec{w} \partial \vec{w}^T}=\vec{\phi}^T\vec{\phi}
\]
\[
\hat{\vec{w}}_{OLS}=\left(\vec{\phi}^T\vec{\phi}\right)^{-1}\vec{\phi}^T\vec{t}
\]
\end{document}